\FILE{section-technologies.tex}


\subsection{Enableing Technology Concepts and Terminology}

A umber of technologies are enabeling us to develop the framework we
describe here. They provide the conrenerstone of our efforts.

\begin{description}

\item[Cloud]
     According to NIST, cloud computing ``is a model for enabling
     ubiquitous, convenient, on-demand network access to a shared pool
     of configurable computing resources (e.g., networks, servers,
     storage, applications, and services) that can be rapidly
     provisioned and released with minimal management effort or
     service provider interaction.`` The model is composed of five
     characteristics adressing together {\em on-demand self-service},
     {\em broad network access], {\em resource pooling}, {\em rapid
     elasticity}, {\em measured services}, {\em software as a
     service}, {\em platform as a service}, {\em infrastructure as a
     service}, and {\em private, public, hybrid cloud},

\item[Hyperscale cloud compute centers]
     provide compute centers that are scaled based on increased demand
     by the user. Hence user have seamingly access to resources they
     require. SUch centers continiously update servere, network, power
     and other resources to meet demand while offering services for
     rent.

\item[Leadership class computing facilities] 
     In the US and also world wide \cite{www-top500} government
     agencies have worked towards makong Leadership Class Computing
     Facility (LCCF) available to the research community. Such
     facilities provide a large-scale computing and data resource. In
     the US ther are funded by DOE and NSF as well as other
     agencies. WHile the first exascale computer has been deliverd at
     ORNL \cite{www-top500} other systems will become online over the
     next 3 years.  Together the LCCF will comprise an {\em ecosystem
     for very large-scale computing in support of promoting progress
     in science.''} They are expected to deliver a significant boost
     in the capable computing power adressing some of the grand
     challanges. As such systems are complex and researchers desire
     ease of access, a service model provides one way of accessing
     them.

\item[Representational state transfer (REST)]
     ``is a software architectural style that was created to guide the
     design and development of the architecture for the World Wide
     Web. REST defines a set of constraints for how the architecture
     of an Internet-scale distributed hypermedia system, such as the
     Web, should behave. The REST architectural style emphasises the
     scalability of interactions between components, uniform
     interfaces, independent deployment of components, and the
     creation of a layered architecture to facilitate caching
     components to reduce user-perceived latency, enforce security,
     and encapsulate legacy systems.\cite{www-rest} ``
     \TODO{FIND NON WIKIPEDIA DEF}

\item[Microservices]
     are an architecture style ``to describe a particular way of
     designing software applications as suites of independently
     deployable services. While there is no precise definition of this
     architectural style, there are certain common characteristics
     around organization around business capability, automated
     deployment, intelligence in the endpoints, and decentralized
     control of languages and data. \cite{www-microservices}''

\item[Analytics as a service]
     provides access to subscription-based data analytics software
     through the cloud. Anlytics services may include the
     sophisticated combination of services internaly used to provide
     customized offerings to the users. The range of resource
     requirement including the time needed to obtain an answer could
     vary widely. For long running analytics efforts asynchronous
     services may be offered, allowing to pick up the result of an
     analytics task at a later time.

\item[Data analytics as a service]
     With increased data demands it is important to integarte the data
     storage needs to access data needed by an analytics service. In
     case of large data needs it is often impractical to move the data
     to a new service. Hence the analytics is often conducted as part
     of add on services running close to the data. Often we speek of
     bring the ``caculation to the data.''

\item[Machine and leep learning].
     Machine learning enables to analyse data based on ``learning''
     from the data. Deep learning is a subdiscipline of machine
     learning while introducing sophisticated toolkist and frameworks
     to use muli-layerd neural networks enabeling non-linera
     transformations on the data. The network is trained based on
     input data and new data can be feed to the trained model to
     obtain a predicted output. Such models require extensive data and
     extensive training to be accurate. One of the challanges is to
     find the model and its hyper parameters to adress a particular
     problem.

% \item[Fair principal]

\end{description}

