\FILE{section-technologies.tex}


\subsection{Enableing Technology Concepts and Terminology}

A umber of technologies are enabeling us to develop the framework we
describe here. They provide the conrenerstone of our efforts.

\begin{description}

\item[Cloud]
     According to NIST, cloud computing ``is a model for enabling
     ubiquitous, convenient, on-demand network access to a shared pool
     of configurable computing resources (e.g., networks, servers,
     storage, applications, and services) that can be rapidly
     provisioned and released with minimal management effort or
     service provider interaction.`` The model is composed of five
     characteristics adressing together {\em on-demand self-service},
     {\em broad network access], {\em resource pooling}, {\em rapid
     elasticity}, {\em measured services}, {\em software as a
     service}, {\em platform as a service}, {\em infrastructure as a
     service}, and {\em private, public, hybrid cloud},

\item[Hyperscale cloud compute centers]
     provide compute centers that are scaled based on increased demand
     by the user. Hence user have seamingly access to resources they
     require. SUch centers continiously update servere, network, power
     and other resources to meet demand while offering services for
     rent.

\item[Leadership class computing facilities] 
     In the US and also world wide \cite{www-top500} government
     agencies have worked towards makong Leadership Class Computing
     Facility (LCCF) available to the research community. Such
     facilities provide a large-scale computing and data resource. In
     the US ther are funded by DOE and NSF as well as other
     agencies. WHile the first exascale computer has been deliverd at
     ORNL \cite{www-top500} other systems will become online over the
     next 3 years.  Together the LCCF will comprise an {\em ecosystem
     for very large-scale computing in support of promoting progress
     in science.''} They are expected to deliver a significant boost
     in the capable computing power adressing some of the grand
     challanges. As such systems are complex and researchers desire
     ease of access, a service model provides one way of accessing
     them.

\item[Representational state transfer (REST)]
     ``is a software architectural style that was created to guide the
     design and development of the architecture for the World Wide
     Web. REST defines a set of constraints for how the architecture
     of an Internet-scale distributed hypermedia system, such as the
     Web, should behave. The REST architectural style emphasises the
     scalability of interactions between components, uniform
     interfaces, independent deployment of components, and the
     creation of a layered architecture to facilitate caching
     components to reduce user-perceived latency, enforce security,
     and encapsulate legacy systems.\cite{www-rest} ``
     \TODO{FIND NON WIKIPEDIA DEF}

\item[Microservices]
     are an architecture style ``to describe a particular way of
     designing software applications as suites of independently
     deployable services. While there is no precise definition of this
     architectural style, there are certain common characteristics
     around organization around business capability, automated
     deployment, intelligence in the endpoints, and decentralized
     control of languages and data. \cite{www-microservices}''

\item[Analytics as a service]
     provides access to subscription-based data analytics software
     through the cloud. Anlytics services may include the
     sophisticated combination of services internaly used to provide
     customized offerings to the users. The range of resource
     requirement including the time needed to obtain an answer could
     vary widely. For long running analytics efforts asynchronous
     services may be offered, allowing to pick up the result of an
     analytics task at a later time.

\item[Data analytics as a service]
     With increased data demands it is important to integarte the data
     storage needs to access data needed by an analytics service. In
     case of large data needs it is often impractical to move the data
     to a new service. Hence the analytics is often conducted as part
     of add on services running close to the data. Often we speek of
     bring the ``caculation to the data.''

\item[Machine and leep learning].
     Machine learning enables to analyse data based on ``learning''
     from the data. Deep learning is a subdiscipline of machine
     learning while introducing sophisticated toolkist and frameworks
     to use muli-layerd neural networks enabeling non-linera
     transformations on the data. The network is trained based on
     input data and new data can be feed to the trained model to
     obtain a predicted output. Such models require extensive data and
     extensive training to be accurate. One of the challanges is to
     find the model and its hyper parameters to adress a particular
     problem.

% \item[Fair principal]

\end{description}

\subsection{Scope and Objectives}

NBD-PWG\footnote{\TODO{introduce in the background section. THe
background section has been integrated here. check valitity and fix
somehow.}} is exploring how to extend NBDIF \footnote{\TODO{introduce
in the background section. THe background section has been integrated
here. check valitity and fix somehow.}} for packaging scalable
analytics as services to meet the challenges of today's information
analytics. These services are intended to be reusable, deployable, and
operational for Big Data, High Performance Computing, AI machine
learning (ML), and deep learning (DL) applications, regardless of the
underlying computing environment.

This document explores key focus areas and document level of interest
from industry, government, and academia in extending the NBDIF to
develop scalable analytics as services that are reusable, deployable,
and operational, regardless of the underlying computing environment.
\TODO{Russel: note that the 'string' reusable, deployable, and
  operational was also used in the previous ppg.}


The work has been conducted with input from the NIST BIg Data Working
group while enhancing their original activities to address
requirements for Analytics Service. This includes hosted on
computational resources including Clouds, Containers, and High
Performance Computing (HPC), thus targeting analytics services hosted
on premis, private and public clouds. We have chosen REpresentational
State Transfer (REST) to formulate some details of the architecture,
it is independent from REST and can be formulated in other
frameworks. While using REST we use a familiar pattern for architect,
implementer, and strategists. Due to the many frameworks, programming
languages and services supporting REST the architecture can easily be
enhanced and implemented with various technical solutions.


TBD:

The
analytics framework also targets big data.
Big data is a term used to
describe extensive datasets, primarily in the characteristics of
volume, variety, velocity, and veracity. While opportunities exist
with Big Data analytics, the data characteristics can overwhelm
traditional technical approaches, and the growth of data is outpacing
scientific and technological advances in data analytics.

To advance
progress in Big Data analytics, the NIST Big Data Public Working Group
(NBD-PWG) is working to develop consensus on important fundamental
concepts related to Big Data. The results are reported in the NIST Big
Data Interoperability Framework (NBDIF) series of volumes.

