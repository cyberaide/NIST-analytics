\FILE{section-technologies.tex}


\subsection{Enableing Technology Concepts and Terminology}

A number of technologies are enabling us to develop the framework we
describe here, and they provide the cornerstone of our efforts.

\begin{description}

\item[Cloud]
     According to NIST, cloud computing ``is a model for enabling
     ubiquitous, convenient, on-demand network access to a shared pool
     of configurable computing resources (e.g., networks, servers,
     storage, applications, and services) that can be rapidly
     provisioned and released with minimal management effort or
     service provider interaction.'' The model is composed of five
     characteristics addressing together {\em on-demand self-service},
     {\em broad network access}, {\em resource pooling}, {\em rapid
     elasticity}, {\em measured services}, {\em software as a
     service}, {\em platform as a service}, {\em infrastructure as a
     service}, and {\em private, public, hybrid cloud},

\item[Hyperscale cloud compute centers]
     provide compute centers that are scaled based on increased demand
     by the user. Hence users have seemingly access to resources they
     require. Such centers continuously update server, network, power
     and other resources to meet demand while offering services for
     rent.

\item[Leadership class computing facilities] 
     In the US and also worldwide \cite{www-top500} government
     agencies have worked towards making Leadership Class Computing
     Facility (LCCF) is available to the research community. Such
     facilities provide large-scale computing and data resources. In
     the US, they are funded by DOE, NSF, and other
     agencies. While the first exascale computer was deliverd at
     ORNL \cite{www-top500} other systems will become online over the
     next three years. Together the LCCF will comprise an {\em ecosystem
     for very large-scale computing in support of promoting progress
     in science.''} They are expected to deliver a significant boost
     in the capable computing power, addressing some of the grand
     challenges. As such systems are complex and researchers desire
     ease of access, a service model provides one way of accessing
     them.

\item[Representational state transfer (REST)]
     is a software architectural style in support of the design and 
     design and development of services exposed to the  World Wide
     Web. REST defines a set of 
     constraints for how the services behave while focusing on 
     scalability of interactions between components, uniform
     interfaces, independent deployment of components, and the
     creation of a layered architecture to facilitate caching
     components to reduce user-perceived latency, enforce security,
     and encapsulate legacy systems \cite{www-rest}.

\item[Microservices]
     are an architecture style ``to describe a particular way of
     designing software applications as suites of independently
     deployable services. While there is no precise definition of this
     architectural style, there are certain common characteristics
     around organization around business capability, automated
     deployment, intelligence in the endpoints, and decentralized
     control of languages and data. \cite{www-microservices}''

\item[Analytics as a service]
     provides access to subscription-based data analytics software
     through the cloud. Analytics services may include the
     sophisticated combination of services internally used to provide
     customized offerings to the users. The range of resource
     requirements, including the time needed to obtain an answer, could
     vary widely. For long-running analytics efforts, asynchronous
     services may be offered, allowing to pick up the result of an
     analytics task at a later time.

\item[Data analytics as a service]
     With increased data demands, it is important to integrate the data
     storage needs to access data needed by an analytics service. In
     the case of significant data needs, moving the data
     to a new service is often impractical. Hence the analytics is often conducted as part
     of add-on services running close to the data. This is often referred to as "bringing the calculation to the data.''

\item[Machine and deep learning].
     Machine learning enables analysis of data by  ``learning''
     from the data. Deep learning [DL] is a subdiscipline of machine
     learning. DL introduces sophisticated toolkits and frameworks which use multi-layered neural networks that enable non-linear
     transformations on the data. The network is first trained on
     a subset of data aka input data; and then new data can be fed to the trained model, to
     obtain an output such as a prediction. DL models require extensive data and
     extensive training to be accurate. Another challenge is finding a good model and good hyper parameters to address a particular
     problem.

% \item[Fair principal]

\end{description}

