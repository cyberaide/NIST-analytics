\subsection{Analytics Parameter Study Experiments}

One of the important aspects of analytics as a service is the ability to integrate long-running analytics tasks either 
on the current computer, remote computers, or batch queuing systems from HPC tasks. These tasks can be 
executed directly on the host system but can simplify access for the user by placing them behind REST 
services. This is a common compute pattern as part of parameter studies that explore a variety of 
parameters producing analytics results that can then be explored either in collaboration or competition. 

For this purpose, we use two services. First, the Cloudmesh Compute Coordinator, that coordinates computational tasks onto hybrid heterogeneous resources, and second, the Cloudmesh Experiment Executor, that coordinates how to execute various parameter settings to achieve the desired results with these analytics settings.

Together these services allow the following.

\begin{enumerate}
    \item Provisioning one or multiple compute nodes on which the parameter experiment is conducted.
    \item Create hyperparameters for the analytics calculation suitable for the compute resources.
    \item Prepare the compute resources with the needed data and programs to conduct the analytics 
          functions. This can be done by copying the source to the nodes or using a GitHub repository to 
          obtain the source.
    \item Configuring the system's software to prepare for a benchmark run by installing or compiling the code 
          in a way that is best suited for the resource).
    % \item Start GPU monitoring to begin capturing GPU metadata
    \item Executing the analytics function and capturing the results
\end{enumerate}

This logic is captured as part of the analytics parameter experiment management and is implemented using the Cloudmesh Experiment Executor (cloudmesh-ee) utility specifically targeting execution pattern \cite{repo-cloudmesh-ee}.
This utility provides the ability to specify configurable parameters that perform one-to-one substitutions and a special \textit{experiments} parameter set, which creates a permutation for all parameter values as distinct experiments.

This allows the user to provide a single script with a configuration file containing multiple hyperparameter values and have them expand into hundreds of configurations without having to prepare each configuration manually.
A typical workflow of development is illustrated in Figure \ref{fig:ee} and Figure \ref{fig:ee-submit}.

\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.8\textwidth]{images/sbatch-template.png}
    \caption{Compute Coordinator Generation Workflow}\label{fig:ee}

    \centering
    \includegraphics[width=0.8\textwidth]{images/sbatch-submit.png}
    \caption{Compute Coordinator Submission Workflow}\label{fig:ee-submit}

    \centering
    \includegraphics[width=0.8\textwidth]{images/sbatch.pdf}
    \caption{Compute Coordinator Sumission Workflow}\label{fig:sbatch-2}
\end{figure*}

A more in-depth presentation of these services is provided in \cite{las-2023-ai-workflow,las-2023-escience}.

